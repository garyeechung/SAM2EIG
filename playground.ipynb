{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from sam2.modeling.backbones.image_encoder import ImageEncoder, FpnNeck\n",
    "from sam2.modeling.backbones.hieradet import Hiera\n",
    "from sam2.modeling.memory_attention import MemoryAttention, MemoryAttentionLayer\n",
    "from sam2.modeling.memory_encoder import MemoryEncoder, MaskDownSampler, Fuser, CXBlock\n",
    "from sam2.modeling.position_encoding import PositionEmbeddingSine\n",
    "from sam2.modeling.sam.transformer import RoPEAttention\n",
    "\n",
    "from src.base import SAM2Base\n",
    "from src.datasets.flare import FLAREDataset3D\n",
    "from src.predictor import SAM2VideoPredictor\n",
    "from src.utils import get_eig_from_probs, get_coords_of_tensor_max, get_center_by_erosion, get_labels_from_coords\n",
    "\n",
    "\n",
    "DEVICE = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "CHECKPOINT = \"./model_checkpoints/sam2_hiera_large.pt\"\n",
    "\n",
    "\n",
    "position_encoding = PositionEmbeddingSine(num_pos_feats=256,\n",
    "                                          temperature=10000,\n",
    "                                          normalize=True,\n",
    "                                          scale=None)\n",
    "neck = FpnNeck(position_encoding=position_encoding,\n",
    "               d_model=256, backbone_channel_list=[1152, 576, 288, 144],\n",
    "               fpn_top_down_levels=[2, 3], fpn_interp_model=\"nearest\")\n",
    "trunk = Hiera(embed_dim=144, num_heads=2, stages=[2, 6, 36, 4],\n",
    "              global_att_blocks=[23, 33, 43], window_spec=[8, 4, 16, 8],\n",
    "              window_pos_embed_bkg_spatial_size=[7, 7])\n",
    "image_encoder = ImageEncoder(neck=neck, trunk=trunk, scalp=1)\n",
    "\n",
    "self_attention = RoPEAttention(rope_theta=10000.0, feat_sizes=[32, 32], embedding_dim=256,\n",
    "                               num_heads=1, downsample_rate=1, dropout=0.1)\n",
    "cross_attention = RoPEAttention(rope_theta=10000.0, feat_sizes=[32, 32],\n",
    "                                rope_k_repeat=True, embedding_dim=256, num_heads=1,\n",
    "                                downsample_rate=1, dropout=0.1, kv_in_dim=64)\n",
    "layer = MemoryAttentionLayer(activation=\"relu\", dim_feedforward=2048, dropout=0.1,\n",
    "                             pos_enc_at_attn=False, self_attention=self_attention,\n",
    "                             d_model=256, pos_enc_at_cross_attn_keys=True,\n",
    "                             pos_enc_at_cross_attn_queries=False,\n",
    "                             cross_attention=cross_attention)\n",
    "memory_attention = MemoryAttention(d_model=256, pos_enc_at_input=True,\n",
    "                                   layer=layer, num_layers=4)\n",
    "\n",
    "position_encoding_memory = PositionEmbeddingSine(num_pos_feats=64, normalize=True,\n",
    "                                                 scale=None, temperature=10000)\n",
    "mask_downsampler = MaskDownSampler(kernel_size=3, stride=2, padding=1)\n",
    "layer_fuser = CXBlock(dim=256, kernel_size=7, padding=3, layer_scale_init_value=1e-6,\n",
    "                      use_dwconv=True)\n",
    "fuser = Fuser(layer=layer_fuser, num_layers=2)\n",
    "memory_encoder = MemoryEncoder(out_dim=64, position_encoding=position_encoding_memory,\n",
    "                               mask_downsampler=mask_downsampler, fuser=fuser)\n",
    "\n",
    "predictor = SAM2VideoPredictor(image_encoder=image_encoder,\n",
    "                               memory_attention=memory_attention,\n",
    "                               memory_encoder=memory_encoder,\n",
    "                               num_maskmem=7, image_size=1024,\n",
    "                               # apply scaled sigmoid on mask logits for memory encoder,\n",
    "                               # and directly feed input mask as output mask\n",
    "                               sigmoid_scale_for_mem_enc=20.0,\n",
    "                               sigmoid_bias_for_mem_enc=10.0,\n",
    "                               use_mask_input_as_output_without_sam=True,\n",
    "                               # Memory\n",
    "                               directly_add_no_mem_embed=True,\n",
    "                               # use high-resolution feature map in the SAM mask decoder\n",
    "                               use_high_res_features_in_sam=True,\n",
    "                               # 3 masks on the first click on initial conditioning frames\n",
    "                               multimask_output_in_sam=True,\n",
    "                               # SAM heads\n",
    "                               iou_prediction_use_sigmoid=True,\n",
    "                               # cross-attend to object pointers from other frames\n",
    "                               # (based on SAM output tokens) in the encoder\n",
    "                               use_obj_ptrs_in_encoder=True,\n",
    "                               add_tpos_enc_to_obj_ptrs=False,\n",
    "                               only_obj_ptrs_in_the_past_for_eval=True,\n",
    "                               # object occlusion prediction\n",
    "                               pred_obj_scores=True,\n",
    "                               pred_obj_scores_mlp=True,\n",
    "                               fixed_no_obj_ptr=True,\n",
    "                               # multimask tracking settings\n",
    "                               multimask_output_for_tracking=True,\n",
    "                               use_multimask_token_for_obj_ptr=True,\n",
    "                               multimask_min_pt_num=0, multimask_max_pt_num=1000,\n",
    "                               use_mlp_for_obj_ptr_proj=True,\n",
    "                               # Compilation flag\n",
    "                               compile_image_encoder=False)\n",
    "sd = torch.load('model_checkpoints/sam2_hiera_large.pt', map_location='cpu')['model']\n",
    "missing_keys, unexpected_keys = predictor.load_state_dict(sd)\n",
    "print(f\"missing keys: {missing_keys}; unexpected keys: {unexpected_keys}\")\n",
    "predictor = predictor.to(DEVICE)\n",
    "\n",
    "\n",
    "dataset = FLAREDataset3D(data_dir=\"data/FLARE\", image_size=(125, 1024, 1024), warmstart=True)\n",
    "CLASS_ID = 0\n",
    "SAMPLE_ID = 0\n",
    "sample = dataset[SAMPLE_ID]\n",
    "print(f\"image: {sample['image'].shape}; min: {sample['image'].min()}; max: {sample['image'].max()}\")\n",
    "print(f\"masks: {sample['masks'].shape}\")\n",
    "print(f\"coords: {sample['point_coords'].shape}\")\n",
    "print(f\"labels: {sample['point_labels'].shape}, {sample['point_labels']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "images = sample['image'].squeeze(0).permute(1, 0, 2, 3).repeat(1, 3, 1, 1)\n",
    "\n",
    "# images = torch.flip(images, dims=[0])\n",
    "\n",
    "inference_state = {\n",
    "    \"images\": images.to(DEVICE),\n",
    "    \"num_frames\": len(images),\n",
    "    \"offload_video_to_cpu\": True,\n",
    "    \"offload_state_to_cpu\": True,\n",
    "    \"video_height\": images.shape[2],\n",
    "    \"video_width\": images.shape[3],\n",
    "    \"device\": DEVICE,\n",
    "    \"storage_device\": DEVICE,\n",
    "    \"point_inputs_per_obj\": {},\n",
    "    \"mask_inputs_per_obj\": {},\n",
    "    \"cached_features\": {},\n",
    "    \"constants\": {},\n",
    "    \"obj_id_to_idx\": OrderedDict(),\n",
    "    \"obj_idx_to_id\": OrderedDict(),\n",
    "    \"obj_ids\": [],\n",
    "    \"output_dict\": {\n",
    "        \"cond_frame_outputs\": {},\n",
    "        \"non_cond_frame_outputs\": {},\n",
    "    },\n",
    "    \"output_dict_per_obj\": {},\n",
    "    \"temp_output_dict_per_obj\": {},\n",
    "    \"consolidated_frame_inds\": {\n",
    "        \"cond_frame_outputs\": set(),\n",
    "        \"non_cond_frame_outputs\": set()\n",
    "    },\n",
    "    \"tracking_has_started\": False,\n",
    "    \"frames_already_tracked\": {}\n",
    "}\n",
    "\n",
    "predictor.reset_state(inference_state)\n",
    "\n",
    "\n",
    "predictor._get_image_feature(inference_state, frame_idx=0, batch_size=1)\n",
    "# predictor.reset_state(inference_state)\n",
    "\n",
    "ann_obj_id = CLASS_ID\n",
    "# ann_frame_idx = int(sample['point_coords'][ann_obj_id, 0, 0].numpy())\n",
    "# mask = sample['masks'][CLASS_ID, ann_frame_idx].numpy()\n",
    "# print(f\"ann_frame_idx: {ann_frame_idx}\")\n",
    "\n",
    "# points = sample['point_coords'][ann_obj_id, :, 1:].flip(-1).numpy()\n",
    "# labels = sample['point_labels'][ann_obj_id].numpy()\n",
    "# print(f\"points: {points}; labels: {labels}\")\n",
    "\n",
    "\n",
    "# img = images[ann_frame_idx].permute(1, 2, 0).numpy()\n",
    "# img_min, img_max = img.min(), img.max()\n",
    "# img = (img - img_min) / (img_max - img_min)\n",
    "# plt.imshow(img)\n",
    "# plt.contour(mask, levels=[0.1], colors='b', linewidths=.5)\n",
    "# plt.scatter(points[0, 0], points[0, 1], c=\"green\" if labels[0] else \"red\")\n",
    "# plt.show()\n",
    "\n",
    "indices = sample['point_coords'][ann_obj_id, :, 0].numpy().astype(int)\n",
    "# print(f\"indices: {indices}\")\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "for i, idx in enumerate(indices):\n",
    "    img = images[idx].permute(1, 2, 0).numpy()\n",
    "    img_min, img_max = img.min(), img.max()\n",
    "    img = (img - img_min) / (img_max - img_min)\n",
    "    plt.subplot(1, 3, i+1)\n",
    "    plt.imshow(img)\n",
    "\n",
    "    mask = sample['masks'][CLASS_ID, idx].numpy()\n",
    "    plt.contour(mask, levels=[0.1], colors='b', linewidths=.5)\n",
    "\n",
    "    point = sample['point_coords'][ann_obj_id, i:i+1, 1:].flip(-1).numpy()\n",
    "    label = sample['point_labels'][ann_obj_id, i:i+1].numpy()\n",
    "    plt.scatter(point[0, 0], point[0, 1], c=\"green\" if label else \"red\", marker='+')\n",
    "    \n",
    "    plt.axis('off')\n",
    "    plt.title(f\"frame {idx}\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigs = dict()\n",
    "for i, idx in enumerate(indices[::-1]):\n",
    "    points = sample['point_coords'][ann_obj_id, i:i+1, 1:].flip(-1).numpy()\n",
    "    labels = sample['point_labels'][ann_obj_id, i:i+1].numpy()\n",
    "    _, out_obj_ids, out_mask_logits, multimask = predictor.add_new_points_or_box(\n",
    "        inference_state=inference_state,\n",
    "        frame_idx=idx,\n",
    "        obj_id=ann_obj_id,\n",
    "        points=points,\n",
    "        labels=labels,\n",
    "    )\n",
    "    eigs[idx] = get_eig_from_probs(torch.sigmoid(multimask).cpu())\n",
    "\n",
    "    \n",
    "\n",
    "video_segments = {}  # video_segments contains the per-frame segmentation results\n",
    "for out_frame_idx, out_obj_ids, out_mask_logits in predictor.propagate_in_video(inference_state, reverse=True):\n",
    "    video_segments[out_frame_idx] = {\n",
    "        out_obj_id: (out_mask_logits[i]).cpu().numpy()\n",
    "        for i, out_obj_id in enumerate(out_obj_ids)\n",
    "    }\n",
    "\n",
    "for out_frame_idx, out_obj_ids, out_mask_logits in predictor.propagate_in_video(inference_state):\n",
    "    video_segments[out_frame_idx] = {\n",
    "        out_obj_id: (out_mask_logits[i]).cpu().numpy()\n",
    "        for i, out_obj_id in enumerate(out_obj_ids)\n",
    "    }\n",
    "\n",
    "\n",
    "for key in inference_state[\"output_dict\"][\"non_cond_frame_outputs\"].keys():\n",
    "    eig = get_eig_from_probs(torch.sigmoid(inference_state[\"output_dict\"][\"non_cond_frame_outputs\"][key][\"pred_multimasks\"]))\n",
    "    eigs[key] = eig.cpu()\n",
    "\n",
    "eigs = [eigs[key] for key in sorted(list(eigs.keys()))]\n",
    "eigs = torch.stack(eigs, axis=2)\n",
    "eigs = eigs.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masks = {key: val[\"pred_masks\"].detach().cpu()\n",
    "         for key, val in inference_state['output_dict_per_obj'][ann_obj_id][\"cond_frame_outputs\"].items()}\n",
    "for key, val in inference_state[\"output_dict\"][\"non_cond_frame_outputs\"].items():\n",
    "    masks[key] = val[\"pred_masks\"].detach().cpu()\n",
    "masks = [torch.sigmoid(masks[k]) for k in sorted(list(masks.keys()))]\n",
    "masks = torch.cat(masks, dim=1)\n",
    "masks = F.interpolate(masks, size=(1024, 1024), mode='bilinear', align_corners=False).numpy()[0]\n",
    "print(masks.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# _, out_obj_ids, out_mask_logits, multimask = predictor.add_new_points_or_box(\n",
    "#     inference_state=inference_state,\n",
    "#     frame_idx=ann_frame_idx,\n",
    "#     obj_id=ann_obj_id,\n",
    "#     points=points,\n",
    "#     labels=labels,\n",
    "# )\n",
    "# eig = get_eig_from_probs(torch.sigmoid(multimask))\n",
    "\n",
    "# plt.subplot(1, 2, 1)\n",
    "# plt.imshow(torch.sigmoid(out_mask_logits)[0][0].detach().cpu().numpy(), cmap='gray', vmin=0, vmax=1)\n",
    "# plt.subplot(1, 2, 2)\n",
    "# plt.imshow(eig[0][0].detach().cpu().numpy())\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(25, 5))\n",
    "for i, mask in enumerate(masks):\n",
    "    \n",
    "    plt.subplot(5, 25, i + 1)\n",
    "    plt.imshow(mask, cmap='gray', vmin=0, vmax=1)\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 16))\n",
    "for i, prompt_idx in enumerate(indices):\n",
    "    for idx in range(prompt_idx-2, prompt_idx+3):\n",
    "        plt.subplot(6, 5, i*10 + idx - prompt_idx + 3)\n",
    "        plt.imshow(masks[idx], cmap='gray', vmin=0, vmax=1)\n",
    "        plt.axis('off')\n",
    "        if idx == prompt_idx:\n",
    "            plt.scatter(sample[\"point_coords\"][ann_obj_id, i, 2], sample[\"point_coords\"][ann_obj_id, i, 1], c=\"#FF00FF\", marker='+')\n",
    "            plt.title(f\"Prompted Slice: {idx}\")\n",
    "        else:\n",
    "            plt.title(f\"Slice: {idx}\")\n",
    "\n",
    "\n",
    "        plt.subplot(6, 5, i*10 + 5 + idx - prompt_idx + 3)\n",
    "        plt.imshow(eigs[0, idx].detach().cpu().numpy(), vmin=0, vmax=.4771)\n",
    "        plt.axis('off')\n",
    "        if idx == prompt_idx:\n",
    "            plt.scatter(sample[\"point_coords\"][ann_obj_id, i, 2], sample[\"point_coords\"][ann_obj_id, i, 1], c=\"#FF00FF\", marker='+')\n",
    "            plt.title(f\"Prompted Slice: {idx}\")\n",
    "        else:\n",
    "            plt.title(f\"Slice: {idx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_eig_slice, coord_i, coord_j = get_coords_of_tensor_max(eigs[None]).detach().cpu().numpy()[0, 0]\n",
    "\n",
    "plt.figure(figsize=(12, 12))\n",
    "for idx in range(max_eig_slice-4, max_eig_slice+5):\n",
    "    # print(f\"frame {idx}\")\n",
    "    plt.subplot(3, 3, idx - max_eig_slice+5)\n",
    "    plt.imshow(eigs[0, idx].detach().cpu().numpy(), vmin=0, vmax=.4771)\n",
    "    plt.axis('off')\n",
    "\n",
    "    mask = sample[\"masks\"][CLASS_ID, idx].numpy()\n",
    "    plt.contour(mask, levels=[0.1], colors='b', linewidths=.5)\n",
    "\n",
    "    if idx == max_eig_slice:\n",
    "        plt.scatter(coord_j, coord_i, c=\"#FF00FF\", marker='+')\n",
    "        plt.title(f\"Max EIG Slice: {idx}\")\n",
    "    else:\n",
    "        plt.title(f\"Slice: {idx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy as sp\n",
    "\n",
    "alpha = .998\n",
    "beta = .999\n",
    "prob = sp.stats.beta.pdf(np.linspace(0, 1, 100), alpha, beta)\n",
    "plt.plot(prob)\n",
    "plt.xticks(np.linspace(0, 100, 11), np.linspace(0, 1, 11).round(2))\n",
    "plt.xlabel\n",
    "plt.show()\n",
    "\n",
    "res = 50\n",
    "alphas = np.zeros((res, res))\n",
    "betas = np.zeros((res, res))\n",
    "min_, max_ = 0, 5\n",
    "values = np.linspace(min_, max_, res)\n",
    "\n",
    "for i in range(res):\n",
    "    alphas[:, i] = values[i]\n",
    "    betas[res-1-i, : ] = values[i]\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(alphas)\n",
    "plt.xlabel(\"$\\\\alpha$\")\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(betas)\n",
    "plt.ylabel(\"$\\\\beta$\")\n",
    "plt.show()\n",
    "\n",
    "entropy = sp.stats.beta.entropy(alphas, betas)\n",
    "\n",
    "plt.imshow(entropy)\n",
    "plt.xlabel(\"$\\\\alpha$\")\n",
    "plt.ylabel(\"$\\\\beta$\")\n",
    "plt.xticks(np.linspace(0, res, 11), np.linspace(int(min_), max_, 11).round(2))\n",
    "plt.yticks(np.linspace(0, res, 11), np.linspace(max_, int(min_), 11).round(2))\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "# plt.imshow(abs(alphas/(alphas+betas)-.5))\n",
    "plt.imshow(alphas/(alphas+betas))\n",
    "plt.colorbar()\n",
    "# plt.imshow(alphas == betas, cmap='Reds', alpha=.1)\n",
    "plt.plot([0, res-1], [res-1, 0], 'r-', alpha=.3, label=\"$\\\\mathbb{E}(\\\\theta)=0.5$\")\n",
    "plt.xlabel(\"$\\\\alpha$\")\n",
    "plt.ylabel(\"$\\\\beta$\")\n",
    "plt.xticks(np.linspace(0, res, 11), np.linspace(int(min_), max_, 11).round(2))\n",
    "plt.yticks(np.linspace(0, res, 11), np.linspace(max_, int(min_), 11).round(2))\n",
    "plt.title(\"Expected Value\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas/(alphas+betas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "promptsam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
